---
published: false
layout: post
modified: '2022-02-05 19:42 +0530'
comments: true
date: '2022-02-05 19:42 +0530'
categories:
  - Micro services
  - Event driven system
description: >-
  Parquet is a binary file format designed with big data in mind where we must
  access data frequently and efficiently. The way it stores file on the disk is
  also different from other file formats. It is a column-based data file. And in
  reality it uses both row based and column based approach to bring the best of
  both worlds. The data is encoded on disk which ensures that the size remains
  small compared to actual data and is then compressed where the file is scanned
  as whole and cut out redundant parts. The query/read speed is dramatically
  fast when compared to other file formats. Nested data is handled efficiently
  which is quite cumbersome in other file format to achieve. Doesnâ€™t require to
  parse the entire file to find data due to its way of storing data. This makes
  it efficient in reading data. Works quite efficiently with data processing
  frameworks. Automatically stores schema information. SQL querying is possible
  with this file format using
tags:
  - Apache kafka
  - Apache
---
Apache Kafka is a high-throughput, low-latency event processing system designed to handle millions of data feeds per second. It has the capability to store, read and analyze streaming data.
In an object-based system, what we are interested is the objects and its interaction with one another and the state of each object which we may store in a database. In an event-based system, our interest will be more on events generated by those objects or the system than the objects itself. Event data is stored in a structure called log and log is an ordered sequence of these events. Unlike databases, these logs are easy to scale. And Apache Kafka is a system to manage and process these logs.

The main components of Kafka are:
- Topics.
- Kafka connect.
- Kafka streams.

In Apache Kafka terms, these logs are called as Topics, and is stored in a durable way. They are written to disk and are replicated on multiple disks, multiple servers to make sure the data persists even in an event of hardware failure. Topics can be stored for a configurable amount of time ranging from short period of time to forever. Topics can be small or enormous. Each entry of the log item or topics item represent an event in the system.

Modern software systems, unlike traditional where we have a single monolithic application, are microservice based where each service/component can be developed and versioned separately. All these services can talk to each other through Kafka topics. Each service can consume a topic, process it, and generate another topic which can be consumed by another service or stored as a topic.

If you have a non-topic data, then you should be able to connect that to Kafka system via a configurable software package known as Kafka connect. With Kafka connect, you can read in and out data from and to external systems or database. There are a lot of such connectors in market.

Now we have a lot of topics in the system and if you want to do some computation/operation on these topics like aggregation, grouping, filtering, enrichment(joint/connecting topic data), we have an inbuild library/API to do all these and this is called as Kafka streams. The result of it will be another topic.
